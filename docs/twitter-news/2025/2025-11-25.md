Summary:
## News / Update
The week saw rapid shifts across the AI landscape: Anthropic entered computer vision with its first image model, while vLLM cemented itself as a go-to inference engine adopted by major clouds and labs. Robotics headlines included a lawsuit targeting Figure, Uber debuting robot delivery in the UK, and a surge of new funding and tracking advances. Community energy stayed high with hackathon finals drawing top judges and teams, plus roundups spotlighting leading open-model builders and this week’s standout research papers (from new SAM and OLMo releases to fast RL and long-horizon tasks). Google DeepMind published studies probing stronger text–image alignment and the scaling promise of pixel-first training. Google’s Gemini 3 launch buzz even spilled into culture with branded swag amplifying momentum.

## New Tools
A wave of creator- and builder-focused tools arrived. Google’s Nano Banana Pro launched as a free 4K AI photo editor that can turn rough sketches into high-fidelity images with minimal prompting, mimic styles, and even convert slide decks into narrated videos—now accessible in the Gemini app and on the web. WorldGen introduced text-to-3D world generation for fast, multi-stage environment creation. LangChain’s LangSmith Agent Builder brought no-code agent design with guided prompts and built-in memory to non-developers. Glif’s browser-based agent started producing polished, customizable slide decks with transitions and optional voiceover. Open-source builders released a rebuilt, lightning-fast feed tracker with a sleek, Linear-style workflow for snappy filtering and use.

## LLMs
Model progress and leaderboard churn accelerated. Anthropic’s Claude Opus 4.5 arrived cheaper and stronger at practical coding and agentic work, posting top SWE-bench results and quickly displacing Gemini 3 Pro at the peak—though head-to-head sentiment says GPT-5 remains superb at expert-level coding while Gemini 3 excels as an explainer and science communicator. New benchmarks underscored shifting strengths: Kimi-linear-48B outperformed Gemini 3 Pro on long-context multi-needle tasks, and Zyphra’s 760M-parameter ZAYA1 MoE model—built on AMD’s stack—matched or beat larger models in math and coding. Open models advanced too: the P1 family hit International Physics Olympiad gold performance via RL-only training, and Fudan’s DiRL post-training showed an 8B diffusion LM surpassing 32B autoregressive rivals in global reasoning and diversity. Research pushed multimodality and reasoning: OpenMMReasoner offered a flexible path for stronger multimodal reasoning, PathAgent applied stepwise LLM reasoning to pathology images without extra training, and DeepMind reported tighter text–image alignment than expected by adjusting inputs while also exploring pixel-first scaling. Meanwhile, user reactions to Gemini 3 praised noticeable gains in reasoning speed and multimodal competence, with splashy claims like a 130 IQ score reflecting the still-fluid state of evaluation norms.

## Features
Existing platforms rolled out meaningful capability upgrades. Anthropic shipped a suite of agent-building features—Tool Search, Programmatic Tool Calling, and Tool Use Examples—plus a practical prompt tweak (“mgrep”) that doubled the speed and halved token usage for code analysis while improving quality. OpenAI’s Sora added six selectable video prompt styles (Thanksgiving, Vintage, News, Selfie, Comic, Anime) to broaden creative control. Diffusers gained switchable FA3, FA2, and SAGE attention backends compatible with torch.compile for faster, more flexible pipelines. Weaviate streamlined enterprise RAG with a new integration that toggles between internal documents and Google search, and its Query Agent UI now supports real-time concept comparisons for research workflows. Slide Guru’s update now turns generated presentations into narrated videos, enhanced by Nano Banana’s style-transfer capabilities. Developers also highlighted Gemini CLI’s ability to execute large-scale refactors across thousands of files with fewer errors.

## Tutorials & Guides
Learning resources focused on reasoning, prompting, and hands-on RL. Curated picks on spatial intelligence outlined why spatial reasoning may be a next inflection point, while a video deep-dive unpacked challenges in interpreting reasoning models beyond standard interpretability methods. Best-practice guides visualized how to prompt Gemini 3 effectively using Nano Banana Pro. For practical RL, a beginner-friendly notebook lets users train Wordle bots with TRL, OpenEnv, GRPO, and vLLM. An Olmo 3 reimplementation and code walkthrough offered a compact way to study its latest RL training approach. A concise keynote (“War against Slop”) argued for building sharper, more disciplined models—worth watching for practitioners focused on quality.

## Showcases & Demos
Demos showcased both creative flair and real-world utility. Gemini 3 generated a retro-themed website from a single prompt and, via Gemini CLI, executed complex, large-scale code refactors cleanly. Visual creativity surfaced in sketch-driven, physics-aware video generation and in high-detail trim sheet creation powered by Nano Banana Pro. WorldGen’s pipeline built large, interactive 3D worlds from text, while Slide Guru and Glif demonstrated rapid production of attractive slide decks with transitions and narration. Open-source agents took the top spot on Terminal Bench 2 using only Gemini 3, signaling the power of community-led agent frameworks. A pragmatic standout: a custom Delphi-based solution automating vendor and recruiter communications saved a business hundreds of thousands—proof that targeted AI can deliver immediate ROI.

## Discussions & Ideas
Debate and reflection centered on openness, sovereignty, and standards. Industry voices criticized major labs for withholding key eval datasets, warning that closed benchmarks slow collective progress. The sovereignty camp argued that true autonomy means running your own models on your own hardware, not renting intelligence. A spirited discussion questioned Huawei’s bid to standardize a robot OS, weighing unmatched scale against geopolitical toxicity, with Nvidia seen as the main alternative. Builders contrasted coding styles—Gemini’s concise scripts versus ChatGPT’s safety-heavy verbosity—while others noted that iterative, search-heavy “deep research” can outperform elaborate orchestration. Broader context included the rising price tag of training (around $2M for strong models), the drag of EU regulatory complexity on startups, historical threads clarifying who pioneered key deep learning ideas, and continued analysis of top lab papers. Studies from Meta highlighted that ideation diversity boosts research agents’ performance, and debates even crossed into AI-guided genetics with contested heritability estimates. With top models swapping the crown in days, many expect further upheavals week to week.

## Memes & Humor
Two themes resonated with practitioners and fans alike: the evergreen “CUDA out of memory” gut-punch that every ML engineer endures, and the lighthearted culture around model launches—Gemini 3’s swag drop added levity to a week otherwise defined by razor-fast progress.

