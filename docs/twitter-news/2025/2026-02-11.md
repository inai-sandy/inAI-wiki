Summary:
## News / Update
The week brought major industry moves across research, events, and business. Isomorphic Labs unveiled its IsoDDE drug-design engine, claiming more than 2× AlphaFold 3’s accuracy with faster discovery of hidden binding pockets and large gains in antibody prediction—positioning AI to compress drug development timelines. Mistral AI announced a global 48-hour hackathon (Feb 28–Mar 1) with $200,000 in prizes, while Arena broadened its role in model assessment with a new enterprise PDF leaderboard and an academic grants program offering up to $50,000 per project. EntireHQ emerged from stealth with $60 million to build an open platform for agent–human collaboration, and CAIS authors were invited to present at the AI Dot Engineer World’s Fair. On the market side, BakerMcKenzie cut 700 roles as AI reshapes legal operations, a fresh Anthropic customer win signaled continued demand for advanced assistants, and Safe Superintelligence Inc. reminded observers it is quietly progressing. Hardware headlines showed consumer GPUs like the RTX 4090 outperforming DGX Spark on practical workloads. Meanwhile, reports of US researchers moving to Europe underscored mounting tensions over academic freedom, and leadership turbulence at xAI raised questions about stability amid rapid model races.

## New Tools
A wave of developer-oriented releases expanded the AI stack. Google introduced Gemini Skills, a library to extend Gemini API and SDK integrations. New agent ops and automation options landed across the ecosystem: devagents-cli now runs fully in the browser via a WebAssembly build; VibeComfy lets agents orchestrate ComfyUI workflows; and OpenResearcher provides a fully offline long-horizon research pipeline. Data and retrieval received upgrades with NextPlaid, a production-ready multi-vector database bundling an ONNX-based late-interaction inference engine. Pydantic’s Monty project teased ultra-low-latency, memory-safe execution sandboxes for agents. Creative tooling arrived with xAI’s Grok Imagine Image Pro on Yupp and Kling 3.0 launching on VEED alongside a cinematic video challenge. MiniMax-M2 debuted as an open-source desktop-control model, Carla-env released a physics-rich embodied environment for training and evaluation, and Artificial Analysis launched a personalized Model Recommender to match users with models by intelligence, speed, and cost.

## LLMs
Model scale, benchmarks, and methods advanced in tandem. New and rumored frontier systems included GLM 5 with doubled parameters and sparse attention for long context, a report that Baidu’s Ernie 5.0 could be the largest Chinese model to date, and a GLM/Qwen wave with Qwen3.5’s hybrid SSM–Transformer MoE designs and strong open-model adoption (GLM‑4.7‑Flash‑GGUF topping UnslothAI downloads). Benchmarks were volatile: GPT 5.3 Codex hit 90% on Next.js tasks; CF‑Div2‑StepFun introduced 53 fresh competitive programming problems; and multiple reports suggested Claude Opus 4.6 sometimes trails earlier variants on AlgoTune while others argue it narrows the gap to OpenAI’s latest. Kimi K2.5 set striking inference records (very low TTFT and high TPS) and rolled out a multimodal API, while Qwen‑Image‑2.0 added text-to-slides and 2K image generation for production workflows. Foundational evaluation and theory progressed with a Stanford–Harvard benchmark of unpublished, proof-requiring math problems intended to test genuine research reasoning; studies deriving neural scaling exponents from language statistics and challenging Chinchilla-style token-per-parameter ratios; and head-to-head comparisons highlighting GPT‑OSS’s stronger unconstrained code/math outputs relative to other open models. Methods that sharpen reasoning and efficiency included self-verification strategies, iGRPO’s draft‑and‑refine self‑feedback loop for RL, ConceptLM’s next‑concept prediction via vector‑quantized vocabularies, a token‑splitting MoE design for faster routing, and infrastructure wins like Unsloth’s 12× MoE training kernels and LLaDA2.1’s diffusion‑MoE speedups—together pointing to faster, cheaper, and more reliable reasoning at scale.

## Features
Flagship platforms shipped meaningful capability upgrades. OpenAI’s Responses API now directly integrates Agent Skills, letting developers package workflows, scripts, and assets the model can discover and execute. ChatGPT’s Deep Research was upgraded with GPT‑5.2, app and connector access, targeted site search, and fullscreen report generation for richer, real-time synthesis. Deepagents rolled out interactive model switching, tool-calling support across LLMs, smarter history summaries, and a pluggable sandbox interface compatible with services like Modal, Daytona, and Runloop. Developer ergonomics improved with a revamped VS Code model picker (search, favorites, deprecations, and deeper model details). Kimi expanded rapidly—adding vision support with updated docs, a multimodal API, and an “Agent Swarm” to coordinate specialist AI teams across creative and research workflows. Vercel rebuilt v0 to address the “last 10%” reliability gap when AI code meets complex production stacks, and Sora’s new Extensions streamline long‑form writing. Broadly, more agents can now write and run code in secure, file‑aware sandboxes, moving automated workflows beyond coding use cases and into general productivity.

## Showcases & Demos
Real-world stress tests and media demos highlighted practical capability. Claude and other agents used NSA’s Ghidra to analyze binaries without source code, hunting for backdoors and server threats in a realistic security workflow. A Mac Studio cluster ran the massive Kimi K‑2.5 model via MLX Distributed, demonstrating that large-scale inference is possible on Apple Silicon. Video generation leapt forward as Seedance/SeeDance 2.0 impressed testers with hyper‑realistic results, pressuring leaders like Sora and Veo, while Kling 3.0’s launch paired with a cinematic video challenge to spur creative showcases. On the multimodal front, MOVA demonstrated tightly synchronized video‑audio generation. Hardware benchmarks showed Nvidia’s consumer RTX 4090 outpacing DGX Spark by multiples on key inference and fine‑tuning tasks, underscoring how accessible hardware can deliver outsized performance for applied AI.

## Discussions & Ideas
Debates centered on where value and limits emerge as systems scale. Industry voices argued space-based data centers miss the point that training, not inference, drives hardware co‑location needs. Workforce commentary suggested AI is reshaping demand toward skilled tool users rather than triggering mass job loss—though high‑profile layoffs indicate uneven impacts. Multiple threads challenged prevailing assumptions: diffusion world models often hallucinate rather than solve complex tasks; scaling laws derived from language statistics and observations deviating from Chinchilla suggest more nuanced data‑compute tradeoffs; and single‑agent coding systems appear capped, with dynamic multi‑agent teams and automated role generation offering a path forward—so long as communication, not just compute, becomes the focus. Safety perspectives from Anthropic likened modern AI failures to complex industrial accidents—more incoherence than explicit misalignment—while researchers pushed toward continual learning in 2026. Strategic takes emphasized data custody as the competitive battleground, argued that creativity cannot be merely interpolated, and forecast a future where AI generates binaries directly, compressing traditional software workflows. Broader shifts saw veteran researchers leaving big labs to found startups, betting that agility and new training stacks can build defensible RL and agentic moats.

## Memes & Humor
Playful speculation swirled around a rumored SpaceX-linked merger as a catch‑all explanation for recent AI market buzz—capturing the community’s tendency to hunt for dramatic narratives amid a flood of announcements.

